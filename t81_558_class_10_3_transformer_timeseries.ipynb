{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YJNQP1WRQFH",
    "outputId": "817aa75f-41e8-41fe-ee05-2ca8c5ac07b5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n",
    "import torch\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RvQ6vUqbE6r6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "names = ['year', 'month', 'day', 'dec_year', 'sn_value',\n",
    "         'sn_error', 'obs_num', 'extra']\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/SN_d_tot_V2.0.csv\",\n",
    "    sep=';', header=None, names=names,\n",
    "    na_values=['-1'], index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'month', 'day', 'dec_year', 'sn_value', 'sn_error', 'obs_num',\n",
       "       'extra'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_values = df[['sn_value', 'sn_error', 'obs_num','extra']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn_value</th>\n",
       "      <th>sn_error</th>\n",
       "      <th>obs_num</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72860</th>\n",
       "      <td>21</td>\n",
       "      <td>1.1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72861</th>\n",
       "      <td>19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72862</th>\n",
       "      <td>17</td>\n",
       "      <td>1.1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72863</th>\n",
       "      <td>12</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72864</th>\n",
       "      <td>11</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72865 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sn_value  sn_error  obs_num  extra\n",
       "0            -1       0.0        0      1\n",
       "1            -1       0.0        0      1\n",
       "2            -1       0.0        0      1\n",
       "3            -1       0.0        0      1\n",
       "4            -1       0.0        0      1\n",
       "...         ...       ...      ...    ...\n",
       "72860        21       1.1       25      0\n",
       "72861        19       1.2       36      0\n",
       "72862        17       1.1       22      0\n",
       "72863        12       0.5       25      0\n",
       "72864        11       0.5       30      0\n",
       "\n",
       "[72865 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kz/9bjckq8x61zg_grhsqhhc0w40000gn/T/ipykernel_22840/3485092275.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_values['sn_value'] = df_values.sn_value.astype('float')\n"
     ]
    }
   ],
   "source": [
    "df_values['sn_value'] = df_values.sn_value.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No non-numeric values found in 'sn_value'.\n"
     ]
    }
   ],
   "source": [
    "# Check if the column contains any strings and print them\n",
    "\n",
    "column_name = 'sn_value'\n",
    "\n",
    "non_numeric_values = df_values[df_values[column_name].apply(lambda x: isinstance(x, str))][column_name]\n",
    "\n",
    "if not non_numeric_values.empty:\n",
    "    print(f\"Non-numeric values in '{column_name}':\")\n",
    "    print(non_numeric_values)\n",
    "else:\n",
    "    print(f\"No non-numeric values found in '{column_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kz/9bjckq8x61zg_grhsqhhc0w40000gn/T/ipykernel_22840/3319485152.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_values['sn_value'] = df_values['sn_value'].astype(float)\n"
     ]
    }
   ],
   "source": [
    "df_values['sn_value'] = df_values['sn_value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sn_value    float64\n",
       "sn_error    float64\n",
       "obs_num       int64\n",
       "extra         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_values.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn_value</th>\n",
       "      <th>sn_error</th>\n",
       "      <th>obs_num</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72860</th>\n",
       "      <td>21.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72861</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72862</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72863</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72864</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72865 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sn_value  sn_error  obs_num  extra\n",
       "0          -1.0       0.0        0      1\n",
       "1          -1.0       0.0        0      1\n",
       "2          -1.0       0.0        0      1\n",
       "3          -1.0       0.0        0      1\n",
       "4          -1.0       0.0        0      1\n",
       "...         ...       ...      ...    ...\n",
       "72860      21.0       1.1       25      0\n",
       "72861      19.0       1.2       36      0\n",
       "72862      17.0       1.1       22      0\n",
       "72863      12.0       0.5       25      0\n",
       "72864      11.0       0.5       30      0\n",
       "\n",
       "[72865 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_values), columns=df_values.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn_value</th>\n",
       "      <th>sn_error</th>\n",
       "      <th>obs_num</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.045726</td>\n",
       "      <td>-1.502628</td>\n",
       "      <td>-0.572843</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.045726</td>\n",
       "      <td>-1.502628</td>\n",
       "      <td>-0.572843</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.045726</td>\n",
       "      <td>-1.502628</td>\n",
       "      <td>-0.572843</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.045726</td>\n",
       "      <td>-1.502628</td>\n",
       "      <td>-0.572843</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.045726</td>\n",
       "      <td>-1.502628</td>\n",
       "      <td>-0.572843</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72860</th>\n",
       "      <td>-0.761959</td>\n",
       "      <td>-1.267897</td>\n",
       "      <td>2.937699</td>\n",
       "      <td>-28.279220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72861</th>\n",
       "      <td>-0.787756</td>\n",
       "      <td>-1.246558</td>\n",
       "      <td>4.482338</td>\n",
       "      <td>-28.279220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72862</th>\n",
       "      <td>-0.813553</td>\n",
       "      <td>-1.267897</td>\n",
       "      <td>2.516434</td>\n",
       "      <td>-28.279220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72863</th>\n",
       "      <td>-0.878045</td>\n",
       "      <td>-1.395932</td>\n",
       "      <td>2.937699</td>\n",
       "      <td>-28.279220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72864</th>\n",
       "      <td>-0.890944</td>\n",
       "      <td>-1.395932</td>\n",
       "      <td>3.639808</td>\n",
       "      <td>-28.279220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72865 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sn_value  sn_error   obs_num      extra\n",
       "0     -1.045726 -1.502628 -0.572843   0.035362\n",
       "1     -1.045726 -1.502628 -0.572843   0.035362\n",
       "2     -1.045726 -1.502628 -0.572843   0.035362\n",
       "3     -1.045726 -1.502628 -0.572843   0.035362\n",
       "4     -1.045726 -1.502628 -0.572843   0.035362\n",
       "...         ...       ...       ...        ...\n",
       "72860 -0.761959 -1.267897  2.937699 -28.279220\n",
       "72861 -0.787756 -1.246558  4.482338 -28.279220\n",
       "72862 -0.813553 -1.267897  2.516434 -28.279220\n",
       "72863 -0.878045 -1.395932  2.937699 -28.279220\n",
       "72864 -0.890944 -1.395932  3.639808 -28.279220\n",
       "\n",
       "[72865 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_scaled.iloc[:(72865-1000)]\n",
    "df_test = df_scaled.iloc[(72865-1000):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn_value</th>\n",
       "      <th>sn_error</th>\n",
       "      <th>obs_num</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.045726</td>\n",
       "      <td>-1.502628</td>\n",
       "      <td>-0.572843</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.045726</td>\n",
       "      <td>-1.502628</td>\n",
       "      <td>-0.572843</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.045726</td>\n",
       "      <td>-1.502628</td>\n",
       "      <td>-0.572843</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.045726</td>\n",
       "      <td>-1.502628</td>\n",
       "      <td>-0.572843</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.045726</td>\n",
       "      <td>-1.502628</td>\n",
       "      <td>-0.572843</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71860</th>\n",
       "      <td>1.263105</td>\n",
       "      <td>1.250128</td>\n",
       "      <td>2.235591</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71861</th>\n",
       "      <td>0.708469</td>\n",
       "      <td>0.204508</td>\n",
       "      <td>1.393061</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71862</th>\n",
       "      <td>0.592383</td>\n",
       "      <td>0.076473</td>\n",
       "      <td>1.393061</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71863</th>\n",
       "      <td>0.476296</td>\n",
       "      <td>0.183169</td>\n",
       "      <td>1.673904</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71864</th>\n",
       "      <td>0.321515</td>\n",
       "      <td>-0.243615</td>\n",
       "      <td>1.252639</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71865 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sn_value  sn_error   obs_num     extra\n",
       "0     -1.045726 -1.502628 -0.572843  0.035362\n",
       "1     -1.045726 -1.502628 -0.572843  0.035362\n",
       "2     -1.045726 -1.502628 -0.572843  0.035362\n",
       "3     -1.045726 -1.502628 -0.572843  0.035362\n",
       "4     -1.045726 -1.502628 -0.572843  0.035362\n",
       "...         ...       ...       ...       ...\n",
       "71860  1.263105  1.250128  2.235591  0.035362\n",
       "71861  0.708469  0.204508  1.393061  0.035362\n",
       "71862  0.592383  0.076473  1.393061  0.035362\n",
       "71863  0.476296  0.183169  1.673904  0.035362\n",
       "71864  0.321515 -0.243615  1.252639  0.035362\n",
       "\n",
       "[71865 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn_value</th>\n",
       "      <th>sn_error</th>\n",
       "      <th>obs_num</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71865</th>\n",
       "      <td>0.115139</td>\n",
       "      <td>0.076473</td>\n",
       "      <td>0.550531</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71866</th>\n",
       "      <td>-0.155730</td>\n",
       "      <td>-0.328972</td>\n",
       "      <td>1.252639</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71867</th>\n",
       "      <td>-0.155730</td>\n",
       "      <td>-0.478347</td>\n",
       "      <td>1.393061</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71868</th>\n",
       "      <td>-0.078339</td>\n",
       "      <td>0.631292</td>\n",
       "      <td>0.971796</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71869</th>\n",
       "      <td>-0.271816</td>\n",
       "      <td>-0.734417</td>\n",
       "      <td>1.814326</td>\n",
       "      <td>0.035362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72860</th>\n",
       "      <td>-0.761959</td>\n",
       "      <td>-1.267897</td>\n",
       "      <td>2.937699</td>\n",
       "      <td>-28.279220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72861</th>\n",
       "      <td>-0.787756</td>\n",
       "      <td>-1.246558</td>\n",
       "      <td>4.482338</td>\n",
       "      <td>-28.279220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72862</th>\n",
       "      <td>-0.813553</td>\n",
       "      <td>-1.267897</td>\n",
       "      <td>2.516434</td>\n",
       "      <td>-28.279220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72863</th>\n",
       "      <td>-0.878045</td>\n",
       "      <td>-1.395932</td>\n",
       "      <td>2.937699</td>\n",
       "      <td>-28.279220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72864</th>\n",
       "      <td>-0.890944</td>\n",
       "      <td>-1.395932</td>\n",
       "      <td>3.639808</td>\n",
       "      <td>-28.279220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sn_value  sn_error   obs_num      extra\n",
       "71865  0.115139  0.076473  0.550531   0.035362\n",
       "71866 -0.155730 -0.328972  1.252639   0.035362\n",
       "71867 -0.155730 -0.478347  1.393061   0.035362\n",
       "71868 -0.078339  0.631292  0.971796   0.035362\n",
       "71869 -0.271816 -0.734417  1.814326   0.035362\n",
       "...         ...       ...       ...        ...\n",
       "72860 -0.761959 -1.267897  2.937699 -28.279220\n",
       "72861 -0.787756 -1.246558  4.482338 -28.279220\n",
       "72862 -0.813553 -1.267897  2.516434 -28.279220\n",
       "72863 -0.878045 -1.395932  2.937699 -28.279220\n",
       "72864 -0.890944 -1.395932  3.639808 -28.279220\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_series(x_series, y_series, n_past, n_future):\n",
    "  #\n",
    "  # n_past ==> no of past observations\n",
    "  #\n",
    "  # n_future ==> no of future observations\n",
    "  #\n",
    "  X, y = list(), list()\n",
    "  for window_start in range(len(x_series)):\n",
    "    past_end = window_start + n_past\n",
    "    future_end = past_end + n_future\n",
    "    if future_end > len(x_series):\n",
    "      break\n",
    "    # slicing the past and future parts of the window\n",
    "    past = x_series[window_start:past_end,:]\n",
    "\n",
    "  for window_start in range(len(y_series)):\n",
    "    past_end = window_start + n_past\n",
    "    future_end = past_end + n_future\n",
    "    if future_end > len(y_series):\n",
    "      break\n",
    "    # slicing the past and future parts of the window\n",
    "    future = y_series[past_end:future_end]\n",
    "\n",
    "\n",
    "    X.append(past)\n",
    "    y.append(future)\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_past = 400\n",
    "n_future = 5\n",
    "n_features = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, y_train = split_series(df_train.values,df_train.sn_value.values, n_past, n_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_test, y_test = split_series(df_test.values,df_test.sn_value.values, n_past, n_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "us_X4qz3NVh9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Data Preprocessing\n",
    "# start_id = max(df[df['obs_num'] == 0].index.tolist()) + 1\n",
    "# df = df[start_id:].copy()\n",
    "# df['sn_value'] = df['sn_value'].astype(float)\n",
    "# df_train = df[df['year'] < 2000]\n",
    "# df_test = df[df['year'] >= 2000]\n",
    "\n",
    "# spots_train = df_train['sn_value'].to_numpy().reshape(-1, 1)\n",
    "# spots_test = df_test['sn_value'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# spots_train = scaler.fit_transform(spots_train).flatten().tolist()\n",
    "# spots_test = scaler.transform(spots_test).flatten().tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvZXHfkGTPX6"
   },
   "source": [
    "Just like we did for LSTM in the previous section, we again break the data into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9utrJg62N1P-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Sequence Data Preparation\n",
    "# SEQUENCE_SIZE = 10\n",
    "\n",
    "# def to_sequences(seq_size, obs):\n",
    "#     x = []\n",
    "#     y = []\n",
    "#     for i in range(len(obs) - seq_size):\n",
    "#         window = obs[i:(i + seq_size)]\n",
    "#         after_window = obs[i + seq_size]\n",
    "#         x.append(window)\n",
    "#         y.append(after_window)\n",
    "#     return torch.tensor(x, dtype=torch.float32).view(-1, seq_size, 1), torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# x_train, y_train = to_sequences(SEQUENCE_SIZE, spots_train)\n",
    "# x_test, y_test = to_sequences(SEQUENCE_SIZE, spots_test)\n",
    "\n",
    "# # Setup data loaders for batch\n",
    "# train_dataset = TensorDataset(x_train, y_train)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# test_dataset = TensorDataset(x_test, y_test)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test  = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup data loaders for batch\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9TI-mjYTm9u"
   },
   "source": [
    "# Position Encoding for Transformers\n",
    "\n",
    "In the realm of the transformer architecture, a pivotal component that ensures the model's success is its ability to consider the sequence's order. Unlike traditional RNNs or LSTMs, which process sequences step-by-step and inherently respect their order, transformers process all tokens in a sequence simultaneously. While this parallel processing significantly boosts computational efficiency and allows for long-range dependencies to be captured more effectively, it also means that transformers, in their native form, are oblivious to the position or order of tokens in a sequence. This is where the concept of positional encoding comes into play.\n",
    "\n",
    "Positional encoding is a mechanism to provide the transformer with information about the position of tokens within a sequence. Essentially, it infuses order information into the otherwise position-agnostic embeddings. By adding positional encodings to the token embeddings before feeding them into the transformer, each token's position in the sequence becomes discernible to the model.\n",
    "\n",
    "Positional encodings are vectors that get added to the embeddings of tokens. The intuition is to design these vectors in such a way that their values or patterns are unique for each position, allowing the model to differentiate between different positions in the sequence.\n",
    "\n",
    "A popular method to generate positional encodings is using sinusoidal functions. For a given position $p$ in the sequence and dimension $d$ of the embedding, the positional encoding is computed as:\n",
    "\n",
    "$$ PE(2,i) = \\sin(\\frac{p}{10000^{2i/d}}) $$\n",
    "$$ PE(2,i+1) = \\cos(\\frac{p}{10000^{2i/d}}) $$\n",
    "\n",
    "Where $i$ is the dimension index. These sinusoidal functions generate values between -1 and 1 and ensure a unique and repeatable pattern for each position.\n",
    "\n",
    "The choice of sinusoidal functions isn't arbitrary. They have two compelling properties:\n",
    "\n",
    "1. They produce values between -1 and 1, making them compatible with most embedding value ranges.\n",
    "2. Their patterns allow the model to extrapolate positions beyond the sequence lengths seen during training.\n",
    "\n",
    "One might wonder, why not just append or add the token's position as an integer to the embedding? The challenge with this approach is scale. Embedding values, especially after being trained, can exist within a specific range, and directly adding large integers (for tokens further down in long sequences) might disrupt the information in the original embeddings.\n",
    "\n",
    "Furthermore, using raw integers wouldn't provide a consistent way for the model to generalize or extrapolate to sequence lengths not seen during training. Sinusoidal functions, in contrast, offer a predictable pattern that aids in such extrapolation.\n",
    "\n",
    "The following code describes a simple implementation of a transformer-based model using PyTorch's built-in functionalities. The **TransformerModel** class encapsulates a transformer-based neural network designed for sequence processing. Upon initialization, the model sets up several components: an encoder to adjust the input data to a desired dimension, a **pos_encoder** to bestow the sequence with positional information, a core **transformer_encoder** comprising several layers to process the sequence, and a **decoder** to produce the final output. As data flows through the model during the forward pass, it undergoes a series of transformations: it's first projected to a higher dimension, then augmented with positional encodings, processed by the transformer layers, and finally, the last token's representation is harnessed to produce the output. An instance of this model is readily created and can be assigned to a computation device for further training or inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "wi-bRiyCN7Cw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJR-CJLeOzpd"
   },
   "source": [
    "# Constructing the Transformer Model\n",
    "\n",
    "The following code constructs the actual transformer-based model for time series prediction. The model is constructed to accept the following parameters.\n",
    "\n",
    "* **input_dim**: The dimension of the input data, in this case we use only one input, the number of sunspots.\n",
    "* **d_model**: The number of features in the transformer model's internal representations (also the size of embeddings). This controls how much a model can remember and process.\n",
    "* **nhead**: The number of attention heads in the multi-head self-attention mechanism.\n",
    "* **num_layers**: The number of transformer encoder layers.\n",
    "dropout: The dropout probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "hoT-VFSdOANz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model definition using Transformer\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=4, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.decoder(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "model = TransformerModel().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi_9wvWWfOGv"
   },
   "source": [
    "The Transformer architecture in PyTorch is governed by crucial configuration choices, among which **d_model**, **nhead**, and **num_layers** hold significant weight. The **d_model** denotes the dimensionality of the input embeddings and affects the model's capacity to learn intricate representations. While a more substantial **d_model** can bolster the richness of the model's understanding, it also amplifies the computational demand and can pose overfitting risks if not carefully chosen. Parallelly, the model's gradient flow and initialization are impacted by this choice, though the Transformer's normalization layers often moderate potential issues.\n",
    "\n",
    "On the other hand, **nhead** reflects the count of heads in the multi-head attention mechanism. A higher number of heads grants the model the prowess to simultaneously focus on diverse segments of the input, enabling the capture of varied contextual nuances. However, there's a trade-off. Beyond a specific threshold, the computational overhead might outweigh the marginal performance gains. This parallel processing, provided by multiple attention heads, tends to offer more stable and varied gradient information, positively influencing the training dynamics.\n",
    "\n",
    "Lastly, the **num_layers** parameter dictates the depth of the Transformer, determining the number of stacked encoder or decoder layers. A deeper model, as a result of increased layers, can discern more complex and hierarchical relationships in data. Still, there's a caveat: after a certain depth, potential performance enhancements may plateau, and the risk of overfitting might escalate. Training deeper models also comes with its set of challenges. Although residual connections and normalization in Transformers alleviate some concerns, a high layer count might necessitate techniques like gradient clipping or learning rate adjustments for stable training.\n",
    "\n",
    "In essence, these parameters intricately balance model capacity, computational efficiency, and generalization capability. Their optimal settings often emerge from task-specific experimentation, the nature of the data, and available computational prowess.\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "\n",
    "Training a transformer-based model adheres to many of the familiar paradigms and best practices that apply to other neural network architectures. Much like the models we've encountered before, a transformer-based model benefits from training in batches, which helps in both computational efficiency and generalization. Batched training ensures that the model updates its weights based on the average gradient over several data points, rather than being excessively influenced by any single instance. Additionally, the use of early stopping acts as a safeguard against overfitting. By monitoring the model's performance on a validation set and halting training when no significant improvement is observed over a set number of epochs, we ensure that the model generalizes well and doesn't just memorize the training data. The validation set, it remains an essential component in the training regimen, providing a proxy measure of the model's performance on unseen data and guiding hyperparameter tuning. Thus, while transformer architectures introduce novel mechanisms and complexities, the foundational principles of training deep learning models in PyTorch remain consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8-mF1OCOLnB",
    "outputId": "35d5c7d3-f1d4-49f3-ed9d-fc11bcf15349",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "epochs = 10\n",
    "early_stop_count = 0\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x_batch, y_batch = batch\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= 5:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS-lkHvOhXGm"
   },
   "source": [
    "We can now evaluate the performance of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dE9fco0GOQr6",
    "outputId": "1591cc01-5e80-4250-db31-1b93317391fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_batch, y_batch = batch\n",
    "        x_batch = x_batch.to(device)\n",
    "        outputs = model(x_batch)\n",
    "        predictions.extend(outputs.squeeze().tolist())\n",
    "\n",
    "rmse = np.sqrt(np.mean((scaler.inverse_transform(np.array(predictions).reshape(-1, 1)) - scaler.inverse_transform(y_test.numpy().reshape(-1, 1)))**2))\n",
    "print(f\"Score (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch_TS] *",
   "language": "python",
   "name": "conda-env-Pytorch_TS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
